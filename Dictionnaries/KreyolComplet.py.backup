#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ‡¬ğŸ‡µ KREYÃ’L POTOMITANâ„¢ - PIPELINE UNIQUE ET AUTOMATIQUE ğŸ‡¬ğŸ‡µ
===========================================================

Le pipeline ultime pour le clavier crÃ©ole intelligent.
EXÃ‰CUTION AUTOMATIQUE COMPLÃˆTE - Aucune interaction requise !

Pipeline automatique intÃ©grÃ©:
â€¢ RÃ©cupÃ©ration donnÃ©es Hugging Face
â€¢ CrÃ©ation/enrichissement dictionnaire  
â€¢ GÃ©nÃ©ration N-grams intelligents
â€¢ Analyse comparative (delta)
â€¢ Statistiques complÃ¨tes avancÃ©es
â€¢ Analyse mots longs dÃ©taillÃ©e
â€¢ Validation intÃ©grale
â€¢ Nettoyage automatique
â€¢ Sauvegarde sÃ©curisÃ©e

Usage simple: python KreyolComplet.py

Fait avec â¤ï¸ pour prÃ©server le KreyÃ²l GuadeloupÃ©en
"""

import json
import re
import os
import shutil
import sys
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path

# Gestion optionnelle des imports
try:
    from datasets import load_dataset
    HAS_DATASETS = True
except ImportError:
    HAS_DATASETS = False

try:
    from dotenv import load_dotenv
    HAS_DOTENV = True
except ImportError:
    HAS_DOTENV = False

class KreyolPotomitanComplet:
    """Classe unique qui gÃ¨re TOUT le systÃ¨me crÃ©ole"""
    
    def __init__(self):
        self.version = "3.0 - Pipeline Unique"
        self.hf_token = None
        self.textes_kreyol = []
        self.dictionnaire_actuel = {}
        self.ngrams_actuels = {}
        self.nouveau_dictionnaire = {}
        self.nouveaux_ngrams = {}
        
        # Chemins des fichiers
        self.chemin_dict = "../android_keyboard/app/src/main/assets/creole_dict.json"
        self.chemin_ngrams = "../android_keyboard/app/src/main/assets/creole_ngrams.json"
        self.dossier_backup = "backups"
        
        # Configuration
        self.debug = False
        
        self._afficher_banniere()
        self._initialiser_environnement()
    
    def _afficher_banniere(self):
        """Affiche la banniÃ¨re du programme"""
        print("=" * 70)
        print("ğŸ‡¬ğŸ‡µ KREYÃ’L POTOMITANâ„¢ - PIPELINE UNIQUE ET AUTOMATIQUE ğŸ‡¬ğŸ‡µ")
        print("=" * 70)
        print(f"Version: {self.version}")
        print(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("ğŸ¯ EXÃ‰CUTION AUTOMATIQUE COMPLÃˆTE")
        print("=" * 70)
    
    def _initialiser_environnement(self):
        """Initialise l'environnement de travail"""
        print("\nğŸ”§ INITIALISATION")
        print("-" * 30)
        
        # Charger la configuration
        self._charger_configuration()
        
        # CrÃ©er les dossiers nÃ©cessaires
        os.makedirs(self.dossier_backup, exist_ok=True)
        
        # Charger les donnÃ©es existantes
        self._charger_donnees_existantes()
        
        print("âœ… Environnement initialisÃ©")
    
    def _charger_configuration(self):
        """Charge la configuration depuis .env"""
        # Chercher .env dans le rÃ©pertoire courant puis dans le parent (racine du projet)
        env_paths = [".env", "../.env", "../../.env"]
        env_found = False
        
        if HAS_DOTENV:
            for env_path in env_paths:
                if os.path.exists(env_path):
                    load_dotenv(env_path)
                    env_found = True
                    print(f"âœ… Configuration .env trouvÃ©e: {env_path}")
                    break
        
        if env_found:
            token = os.getenv('HF_TOKEN') or os.getenv('HF_TOKEN_read_write')
            if token:
                self.hf_token = token
                print("ğŸ”‘ Token Hugging Face configurÃ©")
            else:
                print("âš ï¸ Token Hugging Face non trouvÃ© dans .env")
        else:
            print("âš ï¸ Configuration .env non trouvÃ©e (optionnel)")
    
    def _charger_donnees_existantes(self):
        """Charge les donnÃ©es existantes si disponibles"""
        # Dictionnaire existant
        if os.path.exists(self.chemin_dict):
            try:
                with open(self.chemin_dict, 'r', encoding='utf-8') as f:
                    dict_data = json.load(f)
                self.dictionnaire_actuel = {mot: freq for mot, freq in dict_data}
                print(f"ğŸ“š Dictionnaire existant: {len(self.dictionnaire_actuel)} mots")
            except Exception as e:
                print(f"âš ï¸ Erreur lecture dictionnaire: {e}")
        
        # N-grams existants
        if os.path.exists(self.chemin_ngrams):
            try:
                with open(self.chemin_ngrams, 'r', encoding='utf-8') as f:
                    self.ngrams_actuels = json.load(f)
                predictions = len(self.ngrams_actuels.get("predictions", {}))
                print(f"ğŸ§  N-grams existants: {predictions} prÃ©dictions")
            except Exception as e:
                print(f"âš ï¸ Erreur lecture N-grams: {e}")
    
    def charger_textes_kreyol(self):
        """Charge les textes crÃ©oles depuis Hugging Face ou localement"""
        print("\nğŸ“– CHARGEMENT DES TEXTES CRÃ‰OLES")
        print("-" * 40)
        
        textes_charges = False
        
        # Essayer Hugging Face d'abord
        if HAS_DATASETS:
            try:
                print("ğŸ”„ TÃ©lÃ©chargement depuis Hugging Face...")
                dataset = load_dataset("POTOMITAN/PawolKreyol-gfc", token=self.hf_token)
                
                self.textes_kreyol = []
                for item in dataset["train"]:
                    # Hugging Face utilise "Texte" (avec majuscule)
                    if "Texte" in item and item["Texte"]:
                        # Convertir au format attendu (dictionnaire avec clÃ© "Texte")
                        self.textes_kreyol.append({
                            "Texte": item["Texte"],
                            "Source": item.get("Source", "Hugging Face")
                        })
                    elif "text" in item and item["text"]:
                        # Fallback si format diffÃ©rent
                        self.textes_kreyol.append({
                            "Texte": item["text"],
                            "Source": item.get("source", "Hugging Face")
                        })
                
                if self.textes_kreyol:
                    print(f"âœ… ChargÃ© {len(self.textes_kreyol)} textes depuis Hugging Face")
                    textes_charges = True
                else:
                    print("âš ï¸ Dataset vide sur Hugging Face")
                    
            except Exception as e:
                print(f"âš ï¸ Erreur Hugging Face: {e}")
        
        # Fallback local si Hugging Face Ã©choue
        if not textes_charges:
            chemins_locaux = [
                "PawolKreyol/Textes_kreyol.json",
                "../PawolKreyol/Textes_kreyol.json",
                "textes_kreyol.json"
            ]
            
            for chemin in chemins_locaux:
                if os.path.exists(chemin):
                    try:
                        with open(chemin, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        
                        if isinstance(data, list):
                            self.textes_kreyol = data
                        elif isinstance(data, dict) and "textes" in data:
                            self.textes_kreyol = data["textes"]
                        else:
                            print(f"âš ï¸ Format inattendu dans {chemin}")
                            continue
                        
                        print(f"âœ… ChargÃ© {len(self.textes_kreyol)} textes depuis {chemin}")
                        textes_charges = True
                        break
                        
                    except Exception as e:
                        print(f"âš ï¸ Erreur lecture {chemin}: {e}")
        
        if not textes_charges:
            print("âŒ Aucun texte chargÃ© - Pipeline limitÃ© au dictionnaire existant")
            return False
        
        return True
    
    def creer_dictionnaire(self):
        """CrÃ©e ou enrichit le dictionnaire crÃ©ole"""
        print("\nğŸ“š CRÃ‰ATION DU DICTIONNAIRE")
        print("-" * 35)
        
        if not self.textes_kreyol:
            print("âš ï¸ Aucun texte disponible - utilisation du dictionnaire existant")
            self.nouveau_dictionnaire = self.dictionnaire_actuel.copy()
            return len(self.nouveau_dictionnaire) > 0
        
        print(f"ğŸ” Analyse de {len(self.textes_kreyol)} textes...")
        
        # Compteur pour tous les mots trouvÃ©s
        compteur_mots = Counter()
        
        # Pattern pour extraire les mots crÃ©oles (avec accents)
        pattern_mot = re.compile(r'\b[a-zA-ZÃ Ã¡Ã¢Ã¤Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã²Ã³Ã´Ã¶Ã¹ÃºÃ»Ã¼Ã§Ã±Ã€ÃÃ‚Ã„ÃˆÃ‰ÃŠÃ‹ÃŒÃÃÃÃ’Ã“Ã”Ã–Ã™ÃšÃ›ÃœÃ‡Ã‘\-]{2,}\b')
        
        for texte in self.textes_kreyol:
            # Extraire le contenu textuel (les donnÃ©es sont des objets JSON)
            if isinstance(texte, dict):
                contenu_texte = texte.get("Texte", "")
            else:
                contenu_texte = str(texte) if texte is not None else ""
            
            # VÃ©rifier que le contenu n'est pas vide ou None
            if not contenu_texte:
                continue
                
            mots = pattern_mot.findall(contenu_texte.lower())
            for mot in mots:
                # Nettoyer le mot
                mot = mot.strip('-')
                if len(mot) >= 2:  # Minimum 2 caractÃ¨res
                    compteur_mots[mot] += 1
        
        # Fusionner avec le dictionnaire existant
        for mot, freq_nouvelle in compteur_mots.items():
            freq_existante = self.dictionnaire_actuel.get(mot, 0)
            compteur_mots[mot] = freq_existante + freq_nouvelle
        
        # Ajouter les mots existants non trouvÃ©s dans les nouveaux textes
        for mot, freq in self.dictionnaire_actuel.items():
            if mot not in compteur_mots:
                compteur_mots[mot] = freq
        
        # Convertir en dictionnaire triÃ© par frÃ©quence
        self.nouveau_dictionnaire = dict(compteur_mots.most_common())
        
        nouveaux_mots = len(self.nouveau_dictionnaire) - len(self.dictionnaire_actuel)
        
        print(f"âœ… Dictionnaire crÃ©Ã©:")
        print(f"   - Total mots: {len(self.nouveau_dictionnaire)}")
        print(f"   - Nouveaux mots: {nouveaux_mots}")
        print(f"   - Mots existants: {len(self.dictionnaire_actuel)}")
        
        return True
    
    def creer_ngrams(self):
        """GÃ©nÃ¨re les N-grams pour les prÃ©dictions"""
        print("\nğŸ§  CRÃ‰ATION DES N-GRAMS")
        print("-" * 30)
        
        if not self.textes_kreyol:
            print("âš ï¸ Aucun texte disponible pour les N-grams")
            return False
        
        print("ğŸ”„ GÃ©nÃ©ration des N-grams...")
        
        # Compteurs pour les diffÃ©rents N-grams
        unigrammes = Counter()
        bigrammes = Counter()
        trigrammes = Counter()
        
        # Pattern pour extraire les mots
        pattern_mot = re.compile(r'\b[a-zA-ZÃ Ã¡Ã¢Ã¤Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã²Ã³Ã´Ã¶Ã¹ÃºÃ»Ã¼Ã§Ã±Ã€ÃÃ‚Ã„ÃˆÃ‰ÃŠÃ‹ÃŒÃÃÃÃ’Ã“Ã”Ã–Ã™ÃšÃ›ÃœÃ‡Ã‘\-]{2,}\b')
        
        for texte in self.textes_kreyol:
            # Extraire le contenu textuel (les donnÃ©es sont des objets JSON)
            if isinstance(texte, dict):
                contenu_texte = texte.get("Texte", "")
            else:
                contenu_texte = str(texte) if texte is not None else ""
            
            # VÃ©rifier que le contenu n'est pas vide ou None
            if not contenu_texte:
                continue
                
            mots = [mot.lower().strip('-') for mot in pattern_mot.findall(contenu_texte.lower()) if len(mot.strip('-')) >= 2]
            
            # Unigrammes
            for mot in mots:
                unigrammes[mot] += 1
            
            # Bigrammes
            for i in range(len(mots) - 1):
                bigramme = (mots[i], mots[i + 1])
                bigrammes[bigramme] += 1
            
            # Trigrammes
            for i in range(len(mots) - 2):
                trigramme = (mots[i], mots[i + 1], mots[i + 2])
                trigrammes[trigramme] += 1
        
        # CrÃ©er le modÃ¨le de prÃ©dictions
        predictions = {}
        total_unigrammes = sum(unigrammes.values())
        
        for mot in unigrammes:
            candidats = []
            
            # Chercher les bigrammes commenÃ§ant par ce mot
            for (mot1, mot2), freq in bigrammes.items():
                if mot1 == mot:
                    prob = freq / unigrammes[mot1] if unigrammes[mot1] > 0 else 0
                    candidats.append({"word": mot2, "prob": round(prob, 3)})
            
            # Trier par probabilitÃ© dÃ©croissante et garder les 10 meilleurs
            candidats.sort(key=lambda x: x["prob"], reverse=True)
            if candidats:
                predictions[mot] = candidats[:10]
        
        # CrÃ©er le modÃ¨le final
        self.nouveaux_ngrams = {
            "version": "2.0",
            "type": "ngram_model",
            "branding": "Potomitanâ„¢ - Pipeline Unique",
            "created": datetime.now().isoformat(),
            "predictions": predictions,
            "stats": {
                "total_unigrammes": len(unigrammes),
                "total_bigrammes": len(bigrammes),
                "total_trigrammes": len(trigrammes),
                "mots_avec_predictions": len(predictions)
            }
        }
        
        print(f"âœ… N-grams crÃ©Ã©s:")
        print(f"   - Unigrammes: {len(unigrammes)}")
        print(f"   - Bigrammes: {len(bigrammes)}")
        print(f"   - Trigrammes: {len(trigrammes)}")
        print(f"   - PrÃ©dictions: {len(predictions)}")
        
        return True
    
    def analyser_delta(self):
        """Analyse les diffÃ©rences entre ancien et nouveau"""
        print("\nğŸ” ANALYSE COMPARATIVE (DELTA)")
        print("-" * 40)
        
        # Delta dictionnaire
        anciens_mots = set(self.dictionnaire_actuel.keys())
        nouveaux_mots = set(self.nouveau_dictionnaire.keys())
        
        mots_ajoutes = nouveaux_mots - anciens_mots
        mots_supprimes = anciens_mots - nouveaux_mots
        mots_conserves = anciens_mots & nouveaux_mots
        
        print(f"\nğŸ“š DELTA DICTIONNAIRE:")
        print(f"   â• Mots ajoutÃ©s: {len(mots_ajoutes)}")
        print(f"   â– Mots supprimÃ©s: {len(mots_supprimes)}")
        print(f"   ğŸ”„ Mots conservÃ©s: {len(mots_conserves)}")
        
        if mots_ajoutes and len(mots_ajoutes) <= 20:
            print(f"   ğŸ“ Nouveaux mots: {', '.join(sorted(mots_ajoutes)[:10])}")
        
        # Delta N-grams
        anciennes_predictions = set(self.ngrams_actuels.get("predictions", {}).keys())
        nouvelles_predictions = set(self.nouveaux_ngrams.get("predictions", {}).keys())
        
        predictions_ajoutees = nouvelles_predictions - anciennes_predictions
        predictions_supprimees = anciennes_predictions - nouvelles_predictions
        
        print(f"\nğŸ§  DELTA N-GRAMS:")
        print(f"   â• Nouvelles prÃ©dictions: {len(predictions_ajoutees)}")
        print(f"   â– PrÃ©dictions supprimÃ©es: {len(predictions_supprimees)}")
        
        if predictions_ajoutees and len(predictions_ajoutees) <= 20:
            echantillon = list(predictions_ajoutees)[:10]
            print(f"\n   ğŸ“ Ã‰chantillon nouvelles prÃ©dictions:")
            for mot in echantillon:
                preds = self.nouveaux_ngrams["predictions"][mot][:3]
                pred_str = ", ".join([p["word"] for p in preds])
                print(f"      + '{mot}' â†’ {pred_str}")
        
        return True
    
    def analyser_complet(self):
        """Analyse statistique complÃ¨te"""
        print("\nğŸ“Š ANALYSE COMPLÃˆTE")
        print("-" * 25)
        
        # Analyser le dictionnaire
        if self.nouveau_dictionnaire:
            frequences = list(self.nouveau_dictionnaire.values())
            
            print(f"\nğŸ“š ANALYSE DICTIONNAIRE:")
            print(f"   - Total mots: {len(self.nouveau_dictionnaire)}")
            print(f"   - FrÃ©quence min: {min(frequences)}")
            print(f"   - FrÃ©quence max: {max(frequences)}")
            print(f"   - FrÃ©quence moyenne: {sum(frequences) / len(frequences):.1f}")
            
            # Distribution des frÃ©quences
            tres_rares = sum(1 for f in frequences if f == 1)
            rares = sum(1 for f in frequences if 2 <= f <= 5)
            frequents = sum(1 for f in frequences if 6 <= f <= 20)
            tres_frequents = sum(1 for f in frequences if f > 20)
            
            total = len(frequences)
            print(f"   - TrÃ¨s rares (freq=1): {tres_rares} ({tres_rares/total*100:.1f}%)")
            print(f"   - Rares (freq 2-5): {rares} ({rares/total*100:.1f}%)")
            print(f"   - FrÃ©quents (freq 6-20): {frequents} ({frequents/total*100:.1f}%)")
            print(f"   - TrÃ¨s frÃ©quents (freq>20): {tres_frequents} ({tres_frequents/total*100:.1f}%)")
            
            # Top mots
            print(f"\n   ğŸ† TOP 15 MOTS:")
            for i, (mot, freq) in enumerate(list(self.nouveau_dictionnaire.items())[:15], 1):
                print(f"       {i:2d}. {mot:<15} (freq: {freq})")
        
        # Analyser les N-grams
        if self.nouveaux_ngrams and "predictions" in self.nouveaux_ngrams:
            predictions = self.nouveaux_ngrams["predictions"]
            stats = self.nouveaux_ngrams.get("stats", {})
            
            print(f"\nğŸ§  ANALYSE N-GRAMS:")
            for key, value in stats.items():
                label = key.replace("_", " ").title()
                print(f"   - {label}: {value}")
            
            print(f"\n   ğŸ¯ EXEMPLES DE PRÃ‰DICTIONS:")
            exemples = ["ka", "nou", "tÃ©", "an", "yo"]
            for mot in exemples:
                if mot in predictions:
                    preds = predictions[mot][:3]
                    pred_str = ", ".join([f"{p['word']}({p['prob']})" for p in preds])
                    print(f"      '{mot}' â†’ {pred_str}")
        
        return True
    
    def analyser_mots_longs(self):
        """Analyse des mots les plus longs"""
        print("\nğŸ“ ANALYSE DES MOTS LONGS")
        print("-" * 35)
        
        dictionnaire = self.nouveau_dictionnaire if self.nouveau_dictionnaire else self.dictionnaire_actuel
        
        if not dictionnaire:
            print("âŒ Aucun dictionnaire disponible")
            return False
        
        # Analyser les longueurs
        mots_avec_longueur = [(mot, len(mot), freq) for mot, freq in dictionnaire.items()]
        mots_tries = sorted(mots_avec_longueur, key=lambda x: (-x[1], -x[2]))
        
        longueurs = [item[1] for item in mots_avec_longueur]
        
        print(f"ğŸ“š Analyse de {len(dictionnaire)} mots crÃ©oles")
        print(f"ğŸ“Š Longueur: min={min(longueurs)}, max={max(longueurs)}, moy={sum(longueurs)/len(longueurs):.1f}")
        
        # Distribution par catÃ©gories
        categories = {
            'Courts (2-4)': len([x for x in longueurs if 2 <= x <= 4]),
            'Moyens (5-8)': len([x for x in longueurs if 5 <= x <= 8]),
            'Longs (9-14)': len([x for x in longueurs if 9 <= x <= 14]),
            'TrÃ¨s longs (15+)': len([x for x in longueurs if x >= 15])
        }
        
        print(f"\nğŸ“ˆ DISTRIBUTION:")
        for cat, count in categories.items():
            pourcentage = count / len(longueurs) * 100
            print(f"   {cat}: {count:4d} mots ({pourcentage:5.1f}%)")
        
        # Top 10 mots les plus longs
        print(f"\nğŸ† TOP 10 MOTS LES PLUS LONGS:")
        for i, (mot, longueur, freq) in enumerate(mots_tries[:10], 1):
            print(f"   {i:2d}. {mot:<25} ({longueur:2d} char, freq: {freq})")
        
        # Mots exceptionnels
        mots_exceptionnels = [item for item in mots_tries if item[1] >= 15]
        if mots_exceptionnels:
            print(f"\nğŸŒŸ MOTS EXCEPTIONNELS (15+ caractÃ¨res): {len(mots_exceptionnels)}")
            for mot, longueur, freq in mots_exceptionnels[:5]:
                print(f"   â­ {mot} ({longueur} caractÃ¨res)")
        
        return True
    
    def nettoyer_dictionnaire(self):
        """Nettoie le dictionnaire (retire mots d'une lettre, etc.)"""
        print("\nğŸ§¹ NETTOYAGE DU DICTIONNAIRE")
        print("-" * 35)
        
        if not self.nouveau_dictionnaire:
            print("âš ï¸ Aucun dictionnaire Ã  nettoyer")
            return False
        
        dictionnaire_original = self.nouveau_dictionnaire.copy()
        
        # Filtrer les mots trop courts ou invalides
        mots_retires = []
        dictionnaire_nettoye = {}
        
        for mot, freq in dictionnaire_original.items():
            # CritÃ¨res de nettoyage
            if len(mot) < 2:  # Mots trop courts
                mots_retires.append(f"'{mot}' (trop court)")
            elif mot.isdigit():  # Nombres purs
                mots_retires.append(f"'{mot}' (nombre)")
            elif len(mot) == 1:  # Une seule lettre
                mots_retires.append(f"'{mot}' (une lettre)")
            else:
                dictionnaire_nettoye[mot] = freq
        
        self.nouveau_dictionnaire = dictionnaire_nettoye
        
        print(f"ğŸ—‘ï¸ Mots retirÃ©s: {len(mots_retires)}")
        if mots_retires and len(mots_retires) <= 10:
            print(f"   {', '.join(mots_retires)}")
        elif mots_retires:
            print(f"   {', '.join(mots_retires[:10])} ... et {len(mots_retires)-10} autres")
        
        print(f"âœ… Dictionnaire nettoyÃ©: {len(dictionnaire_nettoye)} mots")
        
        return True
    
    def valider_tout(self):
        """Validation complÃ¨te du systÃ¨me"""
        print("\nğŸ” VALIDATION COMPLÃˆTE")
        print("-" * 30)
        
        resultats = {}
        
        # Test 1: Validation dictionnaire
        print("\nğŸ“š Test dictionnaire...")
        if os.path.exists(self.chemin_dict):
            try:
                with open(self.chemin_dict, 'r', encoding='utf-8') as f:
                    dict_data = json.load(f)
                
                if isinstance(dict_data, list) and len(dict_data) > 0:
                    # VÃ©rifier Ã©chantillon
                    erreurs = 0
                    for i, item in enumerate(dict_data[:100]):
                        if not isinstance(item, list) or len(item) != 2:
                            erreurs += 1
                        elif len(item[0]) < 2:
                            erreurs += 1
                    
                    if erreurs < 10:  # TolÃ©rance de 10% d'erreurs
                        resultats["Dictionnaire"] = True
                        print(f"   âœ… {len(dict_data)} mots, {erreurs} erreurs mineures")
                    else:
                        resultats["Dictionnaire"] = False
                        print(f"   âŒ Trop d'erreurs: {erreurs}/100")
                else:
                    resultats["Dictionnaire"] = False
                    print("   âŒ Format invalide")
            except Exception as e:
                resultats["Dictionnaire"] = False
                print(f"   âŒ Erreur: {e}")
        else:
            resultats["Dictionnaire"] = False
            print("   âŒ Fichier non trouvÃ©")
        
        # Test 2: Validation N-grams
        print("\nğŸ§  Test N-grams...")
        if os.path.exists(self.chemin_ngrams):
            try:
                with open(self.chemin_ngrams, 'r', encoding='utf-8') as f:
                    ngrams_data = json.load(f)
                
                if ("predictions" in ngrams_data and 
                    isinstance(ngrams_data["predictions"], dict) and
                    len(ngrams_data["predictions"]) > 0):
                    resultats["N-grams"] = True
                    print(f"   âœ… {len(ngrams_data['predictions'])} prÃ©dictions")
                else:
                    resultats["N-grams"] = False
                    print("   âŒ Structure invalide")
            except Exception as e:
                resultats["N-grams"] = False
                print(f"   âŒ Erreur: {e}")
        else:
            resultats["N-grams"] = False
            print("   âŒ Fichier non trouvÃ©")
        
        # Test 3: Test prÃ©dictions
        print("\nğŸ¯ Test prÃ©dictions...")
        if resultats.get("N-grams", False):
            try:
                with open(self.chemin_ngrams, 'r', encoding='utf-8') as f:
                    ngrams = json.load(f)
                
                exemples_test = ["ka", "nou", "tÃ©", "an"]
                predictions_trouvees = 0
                
                for mot in exemples_test:
                    if mot in ngrams.get("predictions", {}):
                        predictions_trouvees += 1
                
                if predictions_trouvees >= len(exemples_test) * 0.75:  # 75% minimum
                    resultats["PrÃ©dictions"] = True
                    print(f"   âœ… {predictions_trouvees}/{len(exemples_test)} exemples")
                else:
                    resultats["PrÃ©dictions"] = False
                    print(f"   âŒ Seulement {predictions_trouvees}/{len(exemples_test)}")
            except Exception as e:
                resultats["PrÃ©dictions"] = False
                print(f"   âŒ Erreur: {e}")
        else:
            resultats["PrÃ©dictions"] = False
            print("   âŒ N-grams invalides")
        
        # Test 4: IntÃ©gritÃ© fichiers
        print("\nğŸ”’ Test intÃ©gritÃ©...")
        try:
            tailles_ok = True
            if os.path.exists(self.chemin_dict):
                taille_dict = os.path.getsize(self.chemin_dict)
                if taille_dict < 1000:  # Minimum 1KB
                    tailles_ok = False
            
            if os.path.exists(self.chemin_ngrams):
                taille_ngrams = os.path.getsize(self.chemin_ngrams)
                if taille_ngrams < 5000:  # Minimum 5KB
                    tailles_ok = False
            
            resultats["IntÃ©gritÃ©"] = tailles_ok
            if tailles_ok:
                print("   âœ… Tailles fichiers correctes")
            else:
                print("   âŒ Fichiers trop petits")
        except Exception as e:
            resultats["IntÃ©gritÃ©"] = False
            print(f"   âŒ Erreur: {e}")
        
        # RÃ©sumÃ© final
        succes = sum(resultats.values())
        total = len(resultats)
        pourcentage = succes / total * 100
        
        print(f"\nğŸ“‹ RÃ‰SUMÃ‰ VALIDATION:")
        for test, reussi in resultats.items():
            status = "âœ… RÃ‰USSI" if reussi else "âŒ Ã‰CHEC"
            print(f"   {test:15s}: {status}")
        
        print(f"\nğŸ† SCORE: {succes}/{total} ({pourcentage:.0f}%)")
        
        if pourcentage == 100:
            print("ğŸ‰ VALIDATION PARFAITE ! SystÃ¨me prÃªt pour Android.")
        elif pourcentage >= 75:
            print("âœ… Validation rÃ©ussie avec quelques points Ã  amÃ©liorer.")
        else:
            print("âš ï¸ Validation partielle. VÃ©rification nÃ©cessaire.")
        
        return pourcentage >= 75
    
    def sauvegarder_donnees(self):
        """Sauvegarde toutes les donnÃ©es gÃ©nÃ©rÃ©es"""
        print("\nğŸ’¾ SAUVEGARDE DES DONNÃ‰ES")
        print("-" * 35)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # CrÃ©er les backups
        if os.path.exists(self.chemin_dict):
            backup_dict = f"{self.dossier_backup}/creole_dict_backup_{timestamp}.json"
            shutil.copy2(self.chemin_dict, backup_dict)
            print(f"ğŸ“ Backup dictionnaire: {backup_dict}")
        
        if os.path.exists(self.chemin_ngrams):
            backup_ngrams = f"{self.dossier_backup}/creole_ngrams_backup_{timestamp}.json"
            shutil.copy2(self.chemin_ngrams, backup_ngrams)
            print(f"ğŸ“ Backup N-grams: {backup_ngrams}")
        
        # Sauvegarder le nouveau dictionnaire
        if self.nouveau_dictionnaire:
            # Convertir en format liste pour Android
            dict_liste = [[mot, freq] for mot, freq in self.nouveau_dictionnaire.items()]
            
            os.makedirs(os.path.dirname(self.chemin_dict), exist_ok=True)
            with open(self.chemin_dict, 'w', encoding='utf-8') as f:
                json.dump(dict_liste, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… Dictionnaire sauvegardÃ©: {len(dict_liste)} mots")
        
        # Sauvegarder les nouveaux N-grams
        if self.nouveaux_ngrams:
            os.makedirs(os.path.dirname(self.chemin_ngrams), exist_ok=True)
            with open(self.chemin_ngrams, 'w', encoding='utf-8') as f:
                json.dump(self.nouveaux_ngrams, f, ensure_ascii=False, indent=2)
            
            predictions = len(self.nouveaux_ngrams.get("predictions", {}))
            print(f"âœ… N-grams sauvegardÃ©s: {predictions} prÃ©dictions")
        
        return True
    
    def pipeline_automatique(self):
        """ExÃ©cute le pipeline complet automatiquement"""
        print("\nğŸš€ PIPELINE AUTOMATIQUE COMPLET")
        print("=" * 40)
        
        etapes = [
            ("Chargement textes", self.charger_textes_kreyol),
            ("CrÃ©ation dictionnaire", self.creer_dictionnaire),
            ("GÃ©nÃ©ration N-grams", self.creer_ngrams),
            ("Nettoyage donnÃ©es", self.nettoyer_dictionnaire),
            ("Analyse delta", self.analyser_delta),
            ("Analyse complÃ¨te", self.analyser_complet),
            ("Sauvegarde", self.sauvegarder_donnees),
            ("Validation finale", self.valider_tout)
        ]
        
        succes_total = True
        
        for i, (nom, fonction) in enumerate(etapes, 1):
            print(f"\nâ³ Ã‰tape {i}/{len(etapes)}: {nom}")
            try:
                resultat = fonction()
                if resultat:
                    print(f"âœ… {nom} - TerminÃ©")
                else:
                    print(f"âš ï¸ {nom} - Avec avertissements")
            except Exception as e:
                print(f"âŒ {nom} - Erreur: {e}")
                succes_total = False
        
        print(f"\n{'='*40}")
        if succes_total:
            print("ğŸ‰ PIPELINE AUTOMATIQUE TERMINÃ‰ AVEC SUCCÃˆS!")
            print("ğŸ“± Fichiers prÃªts pour l'intÃ©gration Android")
        else:
            print("âš ï¸ Pipeline terminÃ© avec quelques problÃ¨mes")
            print("ğŸ” VÃ©rifiez les messages ci-dessus")
        
        return succes_total

def main():
    """Fonction principale - Pipeline unique automatique"""
    # CrÃ©er l'instance principale
    potomitan = KreyolPotomitanComplet()
    
    print("\nğŸš€ DÃ‰MARRAGE DU PIPELINE UNIQUE KREYÃ’L POTOMITANâ„¢")
    print("=" * 60)
    print("ğŸ¯ ExÃ©cution automatique complÃ¨te avec toutes les statistiques")
    print("=" * 60)
    
    # ExÃ©cuter le pipeline complet automatiquement
    succes = potomitan.pipeline_automatique()
    
    if succes:
        print("\n" + "=" * 60)
        print("ğŸ‰ PIPELINE KREYÃ’L POTOMITANâ„¢ TERMINÃ‰ AVEC SUCCÃˆS!")
        print("=" * 60)
        print("ğŸ“± Fichiers prÃªts pour l'intÃ©gration Android")
        print("ğŸ‡¬ğŸ‡µ KreyÃ²l Gwadloup ka viv! ğŸ‡¬ğŸ‡µ")
        sys.exit(0)
    else:
        print("\n" + "=" * 60)
        print("âŒ PIPELINE TERMINÃ‰ AVEC DES PROBLÃˆMES")
        print("=" * 60)
        print("ğŸ” Consultez les messages ci-dessus pour plus de dÃ©tails")
        sys.exit(1)

if __name__ == "__main__":
    main()