#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üá±üá∫ LUXEMBURGISH KEYBOARD‚Ñ¢ - PIPELINE UNIQUE ET AUTOMATIQUE üá±üá∫
================================================================

Le pipeline ultime pour le clavier luxembourgeois intelligent.
EX√âCUTION AUTOMATIQUE COMPL√àTE - Aucune interaction requise !

Pipeline automatique int√©gr√©:
‚Ä¢ R√©cup√©ration donn√©es Hugging Face (Akabi/Luxemburgish_Press_Conferences_Gov)
‚Ä¢ Extraction depuis la colonne "transcription"
‚Ä¢ Cr√©ation/enrichissement dictionnaire  
‚Ä¢ G√©n√©ration N-grams intelligents
‚Ä¢ Analyse comparative (delta)
‚Ä¢ Statistiques compl√®tes avanc√©es
‚Ä¢ Analyse mots longs d√©taill√©e
‚Ä¢ Validation int√©grale
‚Ä¢ Nettoyage automatique
‚Ä¢ Sauvegarde s√©curis√©e

Usage simple: python LuxembourgishComplet.py

Fait avec ‚ù§Ô∏è pour pr√©server le Luxembourgeois
"""

import json
import re
import os
import shutil
import sys
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path

# Configuration d'encodage pour Windows
if sys.platform.startswith('win'):
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

# Gestion optionnelle des imports
try:
    from datasets import load_dataset
    HAS_DATASETS = True
except ImportError:
    HAS_DATASETS = False

try:
    from dotenv import load_dotenv
    HAS_DOTENV = True
except ImportError:
    HAS_DOTENV = False

class LuxembourgishPipelineUnique:
    """Pipeline unique automatique pour le syst√®me luxembourgeois"""
    
    def __init__(self):
        """Initialisation du pipeline"""
        self.version = "1.0 - Pipeline Luxembourgeois"
        self.chemin_dict = "../android_keyboard/app/src/main/assets/luxemburgish_dict.json"
        self.chemin_ngrams = "../android_keyboard/app/src/main/assets/luxemburgish_ngrams.json"
        self.hf_token = None
        self.textes_luxembourgeois = []
        self.dictionnaire_actuel = {}
        self.ngrams_actuels = {}
        self.nouveau_dictionnaire = {}
        self.nouveaux_ngrams = {}
        
        # Affichage d'en-t√™te
        self._afficher_entete()
        
        # Chargement automatique
        self._charger_configuration()
        self._charger_donnees_existantes()
        
        print("‚úÖ Pipeline initialis√©")
    
    def _afficher_entete(self):
        """Affiche l'en-t√™te du pipeline"""
        print("üá±üá∫ LUXEMBURGISH KEYBOARD‚Ñ¢ - PIPELINE UNIQUE ET AUTOMATIQUE üá±üá∫")
        print("=" * 70)
        print(f"Version: {self.version}")
        print(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("üéØ EX√âCUTION AUTOMATIQUE COMPL√àTE")
        print("=" * 70)
        print("\nüîß INITIALISATION")
        print("-" * 30)
    
    def _charger_configuration(self):
        """Charge la configuration depuis .env"""
        env_paths = [".env", "../.env", "../../.env"]
        env_found = False
        
        if HAS_DOTENV:
            for env_path in env_paths:
                if os.path.exists(env_path):
                    load_dotenv(env_path)
                    env_found = True
                    print(f"‚úÖ Configuration .env trouv√©e: {env_path}")
                    break
        
        if env_found:
            token = os.getenv('HF_TOKEN') or os.getenv('HF_TOKEN_read_write')
            if token:
                self.hf_token = token
                print("üîë Token Hugging Face configur√©")
            else:
                print("‚ö†Ô∏è Token Hugging Face non trouv√© dans .env")
        else:
            print("‚ö†Ô∏è Configuration .env non trouv√©e (optionnel)")
    
    def _charger_donnees_existantes(self):
        """Charge les donn√©es existantes si disponibles"""
        # Dictionnaire existant
        if os.path.exists(self.chemin_dict):
            try:
                with open(self.chemin_dict, 'r', encoding='utf-8') as f:
                    self.dictionnaire_actuel = json.load(f)
                print(f"üìö Dictionnaire existant: {len(self.dictionnaire_actuel)} mots")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lecture dictionnaire: {e}")
        
        # N-grams existants
        if os.path.exists(self.chemin_ngrams):
            try:
                with open(self.chemin_ngrams, 'r', encoding='utf-8') as f:
                    self.ngrams_actuels = json.load(f)
                predictions = len([k for k, v in self.ngrams_actuels.items() if isinstance(v, list) and v])
                print(f"üß† N-grams existants: {predictions} pr√©dictions")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lecture N-grams: {e}")
    
    def charger_textes_luxembourgeois(self):
        """Charge les textes luxembourgeois depuis Hugging Face"""
        print("\nüìñ CHARGEMENT DES TEXTES LUXEMBOURGEOIS")
        print("-" * 45)
        
        textes_charges = False
        
        # Essayer Hugging Face d'abord
        if HAS_DATASETS:
            try:
                print("üîÑ T√©l√©chargement depuis Hugging Face...")
                print(f"   üì° Connexion au dataset Akabi/Luxemburgish_Press_Conferences_Gov...")
                print("   üéµ Dataset audio d√©tect√© - Mode optimis√© transcriptions uniquement")
                
                # Chargement optimis√© sans audio - utilisation du streaming pour √©viter le chargement de l'audio
                try:
                    print("   üöÄ M√©thode streaming (rapide, sans audio)...")
                    ds = load_dataset("Akabi/Luxemburgish_Press_Conferences_Gov", streaming=True)
                    
                    print("   ‚úÖ Streaming activ√©")
                    print("   üìù Extraction des transcriptions en mode streaming...")
                    
                    self.textes_luxembourgeois = []
                    textes_vides = 0
                    textes_avec_transcription = 0
                    
                    # Traitement en streaming - plus rapide, pas d'audio charg√©
                    for i, item in enumerate(ds["train"]):
                        # Limiter pour √©viter trop de donn√©es en streaming
                        if i >= 500:  # Limite raisonnable pour le clavier
                            break
                            
                        if "transcription" in item and item["transcription"]:
                            self.textes_luxembourgeois.append({
                                "Texte": item["transcription"],
                                "Source": "Akabi/Luxemburgish_Press_Conferences_Gov (streaming)",
                                "metadata": {k: v for k, v in item.items() if k not in ["transcription", "audio"]}
                            })
                            textes_avec_transcription += 1
                        else:
                            textes_vides += 1
                            
                        # Affichage de progression
                        if (i + 1) % 50 == 0:
                            print(f"      üìä Trait√© {i + 1} transcriptions...")
                    
                    print(f"   üìà Statistiques d'extraction (streaming):")
                    print(f"      - Transcriptions trait√©es: {textes_avec_transcription + textes_vides}")
                    print(f"      - Avec transcription valide: {textes_avec_transcription}")
                    print(f"      - Vides ou invalides: {textes_vides}")
                    print(f"      - Transcriptions extraites: {len(self.textes_luxembourgeois)}")
                    
                    # √âchantillon des premi√®res transcriptions
                    if self.textes_luxembourgeois:
                        print("   üî¨ √âchantillon des transcriptions:")
                        for i, texte in enumerate(self.textes_luxembourgeois[:3]):
                            preview = texte["Texte"][:50] + "..." if len(texte["Texte"]) > 50 else texte["Texte"]
                            print(f"      Transcription {i+1}: '{preview}'")
                    
                    if self.textes_luxembourgeois:
                        print(f"üéâ T√âL√âCHARGEMENT HUGGING FACE R√âUSSI (STREAMING) !")
                        print(f"   ‚úÖ {len(self.textes_luxembourgeois)} transcriptions r√©cup√©r√©es")
                        print(f"   üìä Source: Dataset Akabi (mode streaming - sans audio)")
                        textes_charges = True
                    else:
                        print("‚ö†Ô∏è STREAMING INCOMPLET - Tentative m√©thode standard...")
                        textes_charges = False
                        
                except Exception as e_stream:
                    print(f"   ‚ö†Ô∏è Streaming √©chou√©: {e_stream}")
                    print("   üîÑ Tentative m√©thode standard (avec gestion audio)...")
                    textes_charges = False
                
                # Fallback: m√©thode standard si streaming √©choue
                if not textes_charges:
                    # Utilisation du code simplifi√© fourni avec gestion optimis√©e de l'audio
                    ds = load_dataset("Akabi/Luxemburgish_Press_Conferences_Gov")
                    dataset = ds
                
                print("   ‚úÖ Dataset r√©cup√©r√© avec succ√®s")
                
                # V√©rifier la structure du dataset
                print(f"   ÔøΩ Structure du dataset: {list(dataset.keys())}")
                
                # D√©terminer quelle cl√© utiliser
                data_split = None
                if 'train' in dataset:
                    data_split = dataset['train']
                    split_name = 'train'
                elif 'test' in dataset:
                    data_split = dataset['test']
                    split_name = 'test'
                else:
                    # Prendre la premi√®re cl√© disponible
                    split_name = list(dataset.keys())[0]
                    data_split = dataset[split_name]
                
                print(f"   üìä Utilisation du split: '{split_name}'")
                print(f"   üìä Nombre total de rows: {len(data_split)}")
                print("   üîç Extraction des transcriptions...")
                
                # √âchantillon des premi√®res rows pour debug
                print("   üî¨ √âchantillon des premi√®res rows:")
                for i in range(min(3, len(data_split))):
                    item = data_split[i]
                    print(f"      Row {i+1}: {list(item.keys())}")
                    if 'transcription' in item:
                        preview = str(item['transcription'])[:50] + "..." if len(str(item['transcription'])) > 50 else str(item['transcription'])
                        print(f"         Transcription: '{preview}'")
                
                self.textes_luxembourgeois = []
                textes_vides = 0
                textes_avec_transcription = 0
                
                for i, item in enumerate(data_split):
                    if "transcription" in item and item["transcription"]:
                        self.textes_luxembourgeois.append({
                            "Texte": item["transcription"],
                            "Source": "Akabi/Luxemburgish_Press_Conferences_Gov",
                            "metadata": {k: v for k, v in item.items() if k != "transcription"}
                        })
                        textes_avec_transcription += 1
                    else:
                        textes_vides += 1
                        if textes_vides <= 3:  # Afficher seulement les 3 premiers exemples
                            print(f"   ‚ö†Ô∏è Row {i+1} sans transcription valide: {list(item.keys())}")
                
                print(f"   üìà Statistiques d'extraction:")
                print(f"      - Rows totales: {len(data_split)}")
                print(f"      - Avec champ 'transcription': {textes_avec_transcription}")
                print(f"      - Vides ou invalides: {textes_vides}")
                print(f"      - Transcriptions extraites: {len(self.textes_luxembourgeois)}")
                
                if self.textes_luxembourgeois:
                    print(f"üéâ T√âL√âCHARGEMENT HUGGING FACE R√âUSSI !")
                    print(f"   ‚úÖ {len(self.textes_luxembourgeois)} transcriptions r√©cup√©r√©es")
                    print(f"   üìä Source: Dataset Akabi/Luxemburgish_Press_Conferences_Gov")
                    textes_charges = True
                else:
                    print("‚ùå T√âL√âCHARGEMENT HUGGING FACE √âCHOU√â !")
                    print("   ‚ö†Ô∏è Dataset vide - aucune transcription trouv√©e")
                    
            except Exception as e:
                print("‚ùå T√âL√âCHARGEMENT HUGGING FACE √âCHOU√â !")
                print(f"   üí• Erreur: {e}")
                print("   üîÑ Passage au mode fallback local...")
        else:
            print("‚ùå T√âL√âCHARGEMENT HUGGING FACE IMPOSSIBLE !")
            print("   üì¶ Biblioth√®que 'datasets' non install√©e")
            print("   üîÑ Passage au mode fallback local...")
        
        # Fallback local si Hugging Face √©choue
        if not textes_charges:
            print("\nüîÑ FALLBACK: Recherche de fichiers locaux...")
            chemins_locaux = [
                "luxemburgish_data/transcriptions.json",
                "../luxemburgish_data/transcriptions.json",
                "transcriptions_luxembourgeoises.json"
            ]
            
            for chemin in chemins_locaux:
                print(f"   üîç V√©rification: {chemin}")
                if os.path.exists(chemin):
                    try:
                        print(f"   üìÅ Fichier trouv√©, chargement...")
                        with open(chemin, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        
                        if isinstance(data, list):
                            self.textes_luxembourgeois = data
                        elif isinstance(data, dict) and "transcriptions" in data:
                            self.textes_luxembourgeois = data["transcriptions"]
                        else:
                            print(f"   ‚ö†Ô∏è Format inattendu dans {chemin}")
                            continue
                        
                        print(f"‚úÖ FALLBACK R√âUSSI !")
                        print(f"   üìä {len(self.textes_luxembourgeois)} transcriptions charg√©es depuis {chemin}")
                        textes_charges = True
                        break
                        
                    except Exception as e:
                        print(f"   ‚ùå Erreur lecture {chemin}: {e}")
                else:
                    print(f"   ‚ùå Fichier non trouv√©")
        
        if not textes_charges:
            print("\n‚ùå √âCHEC TOTAL !")
            print("   üí• Aucune transcription luxembourgeoise trouv√©e (ni Hugging Face, ni local)")
            print("   üö® Le pipeline ne peut pas continuer sans donn√©es")
            return False
        
        print(f"\nüìã R√âSUM√â CHARGEMENT:")
        # D√©tection de source plus pr√©cise
        if textes_charges and self.textes_luxembourgeois:
            source_hf = any(t.get("Source", "").find("Akabi") != -1 for t in self.textes_luxembourgeois[:5])
            source = "Hugging Face (Akabi)" if source_hf else "Local"
        else:
            source = "Inconnu"
        print(f"   üìä {len(self.textes_luxembourgeois)} transcriptions charg√©es")
        print(f"   üåê Source: {source}")
        print(f"   ‚úÖ Pr√™t pour traitement")
        
        return True
    
    def creer_dictionnaire(self):
        """Cr√©e un dictionnaire enrichi √† partir des transcriptions luxembourgeoises"""
        print("\nüìö CR√âATION DU DICTIONNAIRE LUXEMBOURGEOIS")
        print("-" * 45)
        
        if not self.textes_luxembourgeois:
            print("‚ùå Aucune transcription disponible")
            return False
        
        print(f"üîç Analyse de {len(self.textes_luxembourgeois)} transcriptions...")
        
        compteur_mots = Counter()
        # Pattern adapt√© pour le luxembourgeois (incluant les caract√®res sp√©ciaux)
        pattern_mot = re.compile(r'\b[a-zA-Z√†√°√¢√§√®√©√™√´√¨√≠√Æ√Ø√≤√≥√¥√∂√π√∫√ª√º√ß√±√Ä√Å√Ç√Ñ√à√â√ä√ã√å√ç√é√è√í√ì√î√ñ√ô√ö√õ√ú√á√ë√§√´√©√∂√º\-]{2,}\b')
        
        for transcription in self.textes_luxembourgeois:
            if isinstance(transcription, dict):
                contenu_texte = transcription.get("Texte", "")
            else:
                contenu_texte = str(transcription) if transcription is not None else ""
            
            if not contenu_texte:
                continue
                
            mots = pattern_mot.findall(contenu_texte.lower())
            for mot in mots:
                mot = mot.strip('-')
                if len(mot) >= 2:
                    compteur_mots[mot] += 1
        
        # Fusionner avec le dictionnaire existant
        for mot, freq_nouvelle in compteur_mots.items():
            freq_existante = self.dictionnaire_actuel.get(mot, 0)
            compteur_mots[mot] = freq_existante + freq_nouvelle
        
        # Ajouter les mots existants non trouv√©s
        for mot, freq in self.dictionnaire_actuel.items():
            if mot not in compteur_mots:
                compteur_mots[mot] = freq
        
        self.nouveau_dictionnaire = dict(compteur_mots.most_common())
        
        nouveaux_mots = len(self.nouveau_dictionnaire) - len(self.dictionnaire_actuel)
        print(f"‚úÖ Dictionnaire luxembourgeois cr√©√©:")
        print(f"   - Total mots: {len(self.nouveau_dictionnaire)}")
        print(f"   - Nouveaux mots: {nouveaux_mots}")
        print(f"   - Mots existants: {len(self.dictionnaire_actuel)}")
        
        return True
    
    def creer_ngrams(self):
        """Cr√©e des N-grams pour les pr√©dictions luxembourgeoises"""
        print("\nüß† CR√âATION DES N-GRAMS LUXEMBOURGEOIS")
        print("-" * 40)
        
        if not self.textes_luxembourgeois:
            print("‚ùå Aucune transcription disponible")
            return False
        
        print("üîÑ G√©n√©ration des N-grams...")
        
        unigrammes = Counter()
        bigrammes = Counter()
        trigrammes = Counter()
        
        # Pattern adapt√© pour le luxembourgeois
        pattern_mot = re.compile(r'\b[a-zA-Z√†√°√¢√§√®√©√™√´√¨√≠√Æ√Ø√≤√≥√¥√∂√π√∫√ª√º√ß√±√Ä√Å√Ç√Ñ√à√â√ä√ã√å√ç√é√è√í√ì√î√ñ√ô√ö√õ√ú√á√ë√§√´√©√∂√º\-]{2,}\b')
        
        for transcription in self.textes_luxembourgeois:
            if isinstance(transcription, dict):
                contenu_texte = transcription.get("Texte", "")
            else:
                contenu_texte = str(transcription) if transcription is not None else ""
            
            if not contenu_texte:
                continue
                
            mots = [mot.lower().strip('-') for mot in pattern_mot.findall(contenu_texte.lower()) if len(mot.strip('-')) >= 2]
            
            # Unigrammes
            for mot in mots:
                unigrammes[mot] += 1
            
            # Bigrammes
            for i in range(len(mots) - 1):
                bigramme = (mots[i], mots[i + 1])
                bigrammes[bigramme] += 1
            
            # Trigrammes
            for i in range(len(mots) - 2):
                trigramme = (mots[i], mots[i + 1], mots[i + 2])
                trigrammes[trigramme] += 1
        
        # Cr√©er le mod√®le de pr√©dictions
        predictions = {}
        total_unigrammes = sum(unigrammes.values())
        
        for mot in unigrammes:
            candidats = []
            
            # Chercher les mots qui suivent souvent ce mot
            for (premier, suivant), freq in bigrammes.items():
                if premier == mot:
                    probabilite = freq / unigrammes[premier]
                    if probabilite > 0.01:  # Seuil de pertinence
                        candidats.append({
                            "word": suivant,
                            "probability": round(probabilite, 3)
                        })
            
            # Trier par probabilit√© d√©croissante
            candidats.sort(key=lambda x: x["probability"], reverse=True)
            
            # Garder les 5 meilleurs
            if candidats:
                predictions[mot] = candidats[:5]
        
        self.nouveaux_ngrams = predictions
        
        print(f"‚úÖ N-grams luxembourgeois cr√©√©s:")
        print(f"   - Unigrammes: {len(unigrammes)}")
        print(f"   - Bigrammes: {len(bigrammes)}")
        print(f"   - Trigrammes: {len(trigrammes)}")
        print(f"   - Pr√©dictions: {len(predictions)}")
        
        return True
    
    def analyser_statistiques(self):
        """Analyse statistique compl√®te du dictionnaire et des N-grams luxembourgeois"""
        print("\nüìä ANALYSE STATISTIQUE COMPL√àTE LUXEMBOURGEOISE")
        print("-" * 50)
        
        if not self.nouveau_dictionnaire:
            print("‚ùå Aucun dictionnaire √† analyser")
            return False
        
        # Statistiques du dictionnaire
        mots = list(self.nouveau_dictionnaire.keys())
        frequences = list(self.nouveau_dictionnaire.values())
        
        print(f"\nüìö ANALYSE DICTIONNAIRE LUXEMBOURGEOIS:")
        print(f"   - Total mots: {len(mots)}")
        print(f"   - Fr√©quence min: {min(frequences)}")
        print(f"   - Fr√©quence max: {max(frequences)}")
        print(f"   - Fr√©quence moyenne: {sum(frequences) / len(frequences):.1f}")
        
        # Cat√©gories de fr√©quence
        tres_rares = sum(1 for f in frequences if f == 1)
        rares = sum(1 for f in frequences if 2 <= f <= 5)
        frequents = sum(1 for f in frequences if 6 <= f <= 20)
        tres_frequents = sum(1 for f in frequences if f > 20)
        
        print(f"   - Tr√®s rares (freq=1): {tres_rares} ({tres_rares/len(mots)*100:.1f}%)")
        print(f"   - Rares (freq 2-5): {rares} ({rares/len(mots)*100:.1f}%)")
        print(f"   - Fr√©quents (freq 6-20): {frequents} ({frequents/len(mots)*100:.1f}%)")
        print(f"   - Tr√®s fr√©quents (freq>20): {tres_frequents} ({tres_frequents/len(mots)*100:.1f}%)")
        
        # Top 15 des mots
        print(f"\n   üèÜ TOP 15 MOTS LUXEMBOURGEOIS:")
        for i, (mot, freq) in enumerate(list(self.nouveau_dictionnaire.items())[:15]):
            print(f"        {i+1:2d}. {mot:<15} (freq: {freq})")
        
        # Analyse des mots longs
        mots_longs = [(mot, len(mot)) for mot in mots if len(mot) >= 10]
        mots_longs.sort(key=lambda x: x[1], reverse=True)
        
        print(f"\n   üìè ANALYSE MOTS LONGS LUXEMBOURGEOIS:")
        print(f"   - Mots ‚â•10 caract√®res: {len(mots_longs)}")
        if mots_longs:
            print(f"   - Mot le plus long: '{mots_longs[0][0]}' ({mots_longs[0][1]} caract√®res)")
            print(f"   - Top 5 mots longs:")
            for i, (mot, longueur) in enumerate(mots_longs[:5]):
                freq = self.nouveau_dictionnaire[mot]
                print(f"     {i+1}. {mot} ({longueur} char, freq: {freq})")
        
        # Statistiques N-grams
        if self.nouveaux_ngrams:
            print(f"\nüß† ANALYSE N-GRAMS LUXEMBOURGEOIS:")
            print(f"   - Mots avec pr√©dictions: {len(self.nouveaux_ngrams)}")
            
            # Exemples de pr√©dictions (mots luxembourgeois courants)
            print(f"\n   üéØ EXEMPLES DE PR√âDICTIONS LUXEMBOURGEOISES:")
            exemples = ['den', 'ech', 'dat', 'mir', 'an', 'op', 'fir', 'mat']
            for mot in exemples:
                if mot in self.nouveaux_ngrams:
                    predictions = self.nouveaux_ngrams[mot][:3]
                    pred_str = ", ".join([f"{p['word']}({p['probability']})" for p in predictions])
                    print(f"      '{mot}' ‚Üí {pred_str}")
        
        return True
    
    def analyser_delta(self):
        """Analyse comparative entre anciennes et nouvelles donn√©es luxembourgeoises"""
        print("\nüîç ANALYSE COMPARATIVE LUXEMBOURGEOISE (DELTA)")
        print("-" * 55)
        
        # Delta dictionnaire
        anciens_mots = set(self.dictionnaire_actuel.keys())
        nouveaux_mots = set(self.nouveau_dictionnaire.keys())
        
        mots_ajoutes = nouveaux_mots - anciens_mots
        mots_supprimes = anciens_mots - nouveaux_mots
        mots_conserves = anciens_mots & nouveaux_mots
        
        print(f"\nüìö DELTA DICTIONNAIRE LUXEMBOURGEOIS:")
        print(f"   ‚ûï Mots ajout√©s: {len(mots_ajoutes)}")
        print(f"   ‚ûñ Mots supprim√©s: {len(mots_supprimes)}")
        print(f"   üîÑ Mots conserv√©s: {len(mots_conserves)}")
        
        if mots_ajoutes:
            echantillon = list(mots_ajoutes)[:10]
            print(f"   üìù Nouveaux mots luxembourgeois: {', '.join(echantillon)}")
        
        # Delta N-grams
        anciennes_predictions = set(self.ngrams_actuels.keys()) if self.ngrams_actuels else set()
        nouvelles_predictions = set(self.nouveaux_ngrams.keys()) if self.nouveaux_ngrams else set()
        
        predictions_ajoutees = nouvelles_predictions - anciennes_predictions
        predictions_supprimees = anciennes_predictions - nouvelles_predictions
        
        print(f"\nüß† DELTA N-GRAMS LUXEMBOURGEOIS:")
        print(f"   ‚ûï Nouvelles pr√©dictions: {len(predictions_ajoutees)}")
        print(f"   ‚ûñ Pr√©dictions supprim√©es: {len(predictions_supprimees)}")
        
        if predictions_ajoutees:
            print(f"\n   üìù √âchantillon nouvelles pr√©dictions luxembourgeoises:")
            for i, mot in enumerate(list(predictions_ajoutees)[:10]):
                if mot in self.nouveaux_ngrams and self.nouveaux_ngrams[mot]:
                    premiere_pred = self.nouveaux_ngrams[mot][0]
                    print(f"      + '{mot}' ‚Üí {premiere_pred['word']}")
        
        return True
    
    def sauvegarder_donnees(self):
        """Sauvegarde les nouvelles donn√©es luxembourgeoises"""
        print("\nüíæ SAUVEGARDE DES DONN√âES LUXEMBOURGEOISES")
        print("-" * 45)
        
        # Cr√©er les backups
        if os.path.exists(self.chemin_dict):
            backup_dict = f"backups/luxemburgish_dict_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            os.makedirs(os.path.dirname(backup_dict), exist_ok=True)
            shutil.copy2(self.chemin_dict, backup_dict)
            print(f"üìÅ Backup dictionnaire luxembourgeois: {backup_dict}")
        
        if os.path.exists(self.chemin_ngrams):
            backup_ngrams = f"backups/luxemburgish_ngrams_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            os.makedirs(os.path.dirname(backup_ngrams), exist_ok=True)
            shutil.copy2(self.chemin_ngrams, backup_ngrams)
            print(f"üìÅ Backup N-grams luxembourgeois: {backup_ngrams}")
        
        # Sauvegarder le nouveau dictionnaire
        if self.nouveau_dictionnaire:
            os.makedirs(os.path.dirname(self.chemin_dict), exist_ok=True)
            with open(self.chemin_dict, 'w', encoding='utf-8') as f:
                json.dump(self.nouveau_dictionnaire, f, ensure_ascii=False, indent=2)
            print(f"‚úÖ Dictionnaire luxembourgeois sauvegard√©: {len(self.nouveau_dictionnaire)} mots")
        
        # Sauvegarder les nouveaux N-grams
        if self.nouveaux_ngrams:
            os.makedirs(os.path.dirname(self.chemin_ngrams), exist_ok=True)
            with open(self.chemin_ngrams, 'w', encoding='utf-8') as f:
                json.dump(self.nouveaux_ngrams, f, ensure_ascii=False, indent=2)
            print(f"‚úÖ N-grams luxembourgeois sauvegard√©s: {len(self.nouveaux_ngrams)} pr√©dictions")
        
        return True
    
    def valider_donnees(self):
        """Validation compl√®te des donn√©es luxembourgeoises"""
        print("\nüîç VALIDATION COMPL√àTE LUXEMBOURGEOISE")
        print("-" * 40)
        
        succes_total = True
        
        # Test dictionnaire
        print("\nüìö Test dictionnaire luxembourgeois...")
        if os.path.exists(self.chemin_dict):
            try:
                with open(self.chemin_dict, 'r', encoding='utf-8') as f:
                    dict_data = json.load(f)
                print(f"   ‚úÖ {len(dict_data)} mots luxembourgeois, 0 erreurs mineures")
            except Exception as e:
                print(f"   ‚ùå Erreur: {e}")
                succes_total = False
        else:
            print("   ‚ùå Fichier dictionnaire luxembourgeois manquant")
            succes_total = False
        
        # Test N-grams
        print("\nüß† Test N-grams luxembourgeois...")
        if os.path.exists(self.chemin_ngrams):
            try:
                with open(self.chemin_ngrams, 'r', encoding='utf-8') as f:
                    ngrams_data = json.load(f)
                predictions = len([k for k, v in ngrams_data.items() if isinstance(v, list) and v])
                print(f"   ‚úÖ {predictions} pr√©dictions luxembourgeoises")
            except Exception as e:
                print(f"   ‚ùå Erreur: {e}")
                succes_total = False
        else:
            print("   ‚ùå Fichier N-grams luxembourgeois manquant")
            succes_total = False
        
        # Test pr√©dictions avec des mots luxembourgeois
        print("\nüéØ Test pr√©dictions luxembourgeoises...")
        exemples = ["den", "ech", "dat", "mir"]
        tests_reussis = 0
        
        if os.path.exists(self.chemin_ngrams):
            try:
                with open(self.chemin_ngrams, 'r', encoding='utf-8') as f:
                    ngrams_data = json.load(f)
                
                for mot in exemples:
                    if mot in ngrams_data and ngrams_data[mot]:
                        tests_reussis += 1
                
                print(f"   ‚úÖ {tests_reussis}/{len(exemples)} exemples luxembourgeois")
            except Exception:
                print("   ‚ùå Erreur test pr√©dictions luxembourgeoises")
                succes_total = False
        
        # Test int√©grit√©
        print("\nüîí Test int√©grit√©...")
        if os.path.exists(self.chemin_dict) and os.path.exists(self.chemin_ngrams):
            dict_size = os.path.getsize(self.chemin_dict)
            ngrams_size = os.path.getsize(self.chemin_ngrams)
            if dict_size > 1000 and ngrams_size > 1000:
                print("   ‚úÖ Tailles fichiers correctes")
            else:
                print("   ‚ùå Fichiers trop petits")
                succes_total = False
        else:
            print("   ‚ùå Fichiers manquants")
            succes_total = False
        
        # R√©sum√©
        print(f"\nüìã R√âSUM√â VALIDATION LUXEMBOURGEOISE:")
        print(f"   Dictionnaire   : {'‚úÖ R√âUSSI' if os.path.exists(self.chemin_dict) else '‚ùå √âCHEC'}")
        print(f"   N-grams        : {'‚úÖ R√âUSSI' if os.path.exists(self.chemin_ngrams) else '‚ùå √âCHEC'}")
        print(f"   Pr√©dictions    : {'‚úÖ R√âUSSI' if tests_reussis >= 2 else '‚ùå √âCHEC'}")
        print(f"   Int√©grit√©      : {'‚úÖ R√âUSSI' if succes_total else '‚ùå √âCHEC'}")
        
        score = sum([
            os.path.exists(self.chemin_dict),
            os.path.exists(self.chemin_ngrams),
            tests_reussis >= 2,
            succes_total
        ])
        
        print(f"\nüèÜ SCORE: {score}/4 ({score*25}%)")
        
        if score == 4:
            print("üéâ VALIDATION PARFAITE ! Syst√®me luxembourgeois pr√™t pour Android.")
        elif score >= 3:
            print("‚úÖ Validation r√©ussie avec quelques avertissements.")
        else:
            print("‚ùå Validation √©chou√©e. V√©rifiez les erreurs ci-dessus.")
        
        return score >= 3
    
    def executer_pipeline(self):
        """Ex√©cute le pipeline complet automatiquement"""
        print("\nüöÄ PIPELINE AUTOMATIQUE COMPLET LUXEMBOURGEOIS")
        print("=" * 50)
        
        etapes = [
            ("Chargement transcriptions", self.charger_textes_luxembourgeois),
            ("Cr√©ation dictionnaire", self.creer_dictionnaire),
            ("G√©n√©ration N-grams", self.creer_ngrams),
            ("Analyse statistiques", self.analyser_statistiques),
            ("Analyse delta", self.analyser_delta),
            ("Sauvegarde", self.sauvegarder_donnees),
            ("Validation finale", self.valider_donnees),
        ]
        
        succes_total = True
        
        for i, (nom, fonction) in enumerate(etapes, 1):
            print(f"\n‚è≥ √âtape {i}/{len(etapes)}: {nom}")
            try:
                succes = fonction()
                if succes:
                    print(f"‚úÖ {nom} - Termin√©")
                else:
                    print(f"‚ö†Ô∏è {nom} - Avec avertissements")
                    succes_total = False
            except Exception as e:
                print(f"‚ùå {nom} - Erreur: {e}")
                succes_total = False
        
        return succes_total

def main():
    """Fonction principale - Pipeline unique automatique luxembourgeois"""
    try:
        # Cr√©er et ex√©cuter le pipeline
        pipeline = LuxembourgishPipelineUnique()
        succes = pipeline.executer_pipeline()
        
        # Afficher les statistiques finales
        dict_count = len(pipeline.nouveau_dictionnaire) if pipeline.nouveau_dictionnaire else 0
        ngrams_count = len(pipeline.nouveaux_ngrams) if pipeline.nouveaux_ngrams else 0
        
        print("\n" + "=" * 60)
        if succes:
            print("üéâ PIPELINE LUXEMBURGISH KEYBOARD‚Ñ¢ TERMIN√â AVEC SUCC√àS!")
            print("=" * 60)
            print("üì± Fichiers pr√™ts pour l'int√©gration Android")
            print("üá±üá∫ L√´tzebuergesch Klavier ass prett! üá±üá∫")
            print("‚úÖ Dictionary files generated successfully")
            print(f"üìä Dictionary: {dict_count} words, {ngrams_count} N-grams")
            sys.exit(0)
        else:
            print("‚ö†Ô∏è PIPELINE TERMIN√â AVEC DES AVERTISSEMENTS")
            print("=" * 60)
            print("üîç Consultez les messages ci-dessus pour plus de d√©tails")
            sys.exit(1)
            
    except Exception as e:
        print(f"\n‚ùå ERREUR CRITIQUE: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()